{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognition\n",
    "digitrec.py is a Python script that takes an image file containing a handwritten digit and identifies the digit using a supervised learning algorithm. This will be done using the MNIST dataset to train and test the neural network. This script uses [keras](https://github.com/keras-team/keras), a high level neural network API. \n",
    "\n",
    "### Digit Dataset\n",
    "First things first, we need to open and read the MNIST dataset. This has already been covered in the [MNIST dataset notebook](https://github.com/HUGHBRADY/Emerging_Tech/blob/master/MNIST%20Dataset/mnist-dataset.ipynb) in this repository. \n",
    "\n",
    "### Imports & MNIST set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hughballs\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as kr\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import gzip\n",
    "\n",
    "# Open and read all of the MNIST files\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:  \n",
    "    raw_test_img = f.read()\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:  \n",
    "    raw_test_lbl = f.read()\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:  \n",
    "    raw_train_img = f.read()\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:  \n",
    "    raw_train_lbl = f.read()\n",
    "\n",
    "# Next we'll trimodelthe excess bytes at the start and convert the bytes to integers\n",
    "test_img = np.frombuffer(raw_test_img, dtype = np.uint8, offset = 16) / 255\n",
    "test_lbl = np.frombuffer(raw_test_lbl, dtype = np.uint8, offset = 8)\n",
    "train_img = np.frombuffer(raw_train_img, dtype = np.uint8, offset = 16) / 255\n",
    "train_lbl = np.frombuffer(raw_train_lbl, dtype = np.uint8, offset = 8)\n",
    "\n",
    "# Finally we reshape the image arrays\n",
    "test_img  =  test_img.reshape(10000, 784)\n",
    "train_img = train_img.reshape(60000, 784)\n",
    "\n",
    "# Convert class vectors to binary class matrices \n",
    "train_lbl = kr.utils.to_categorical(train_lbl, 10)\n",
    "test_lbl = kr.utils.to_categorical(test_lbl, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras \n",
    "We also need to set up our neural network. Keras' core data structure is a model, which is a way to organize layers. The simpliest of these models is the Sequential model so it's the perfect one to start learning with. \n",
    "\n",
    "### Building the Model\n",
    "The Sequential model is a linear stack of layers. The most important layers are the first and last, or input and output layers. The input layer defines the shape of the input. \n",
    "There is a dense connection between the neurons meaning that every neuron in every layer is connected to every neuron in the neighbouring layers.\n",
    "The output layer has ten neurons that correspond to the possible answers (0 - 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,796,010\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create our keras model\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Layers to add to our sequential model. The first layer defines the input shape\n",
    "# Dropout randomly selects nodes to drop, resulting in a network capable of better generalization\n",
    "model.add(Dense(1000, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))                                                 \n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once these layers are added we must compile the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the loss function, optimizer and metrics and is needed for training\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Once the model has been built and the images have been read in, we have to train the model to recognize the images. The training set of 60000 images are used and passed to the networks first layer of 784 neurons. The training images are sent as input and the training labels are attached as the expected output. Epochs refer to the amount of times the input will be processed, which I've set to 20. Finally the batch size in this case refers the amount of images that will be sent into the neural network at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " 5100/60000 [=>............................] - ETA: 22s - loss: 0.6622 - acc: 0.7937"
     ]
    }
   ],
   "source": [
    "model.fit(train_img, train_lbl, batch_size=100, epochs=20, validation_data=(test_img, test_lbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "Return the results of the loses and the accuracy of the mnist neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_img, test_lbl, verbose=0)\n",
    "\n",
    "# Finally, print the loss and the accuracy\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
